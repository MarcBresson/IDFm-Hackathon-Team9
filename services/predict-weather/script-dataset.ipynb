{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b259774b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading hourly weather data...\n",
      "  ✓ Hourly weather: 33317 rows, 6 cols\n",
      "Loading alert data...\n",
      "  ✓ Alerts: 8378 rows, 5 cols\n",
      "  ✓ Hourly weather: 33317 rows, 6 cols\n",
      "Loading alert data...\n",
      "  ✓ Alerts: 8378 rows, 5 cols\n",
      "\n",
      "======================================================================\n",
      "CREATING PHENOMENA × VIGILANCE PIVOT TABLE WITH WEATHER DATA\n",
      "======================================================================\n",
      "Detected phenomenon column: phenomene_id\n",
      "Detected vigilance column: niveau_vigilance\n",
      "\n",
      "======================================================================\n",
      "CREATING PHENOMENA × VIGILANCE PIVOT TABLE WITH WEATHER DATA\n",
      "======================================================================\n",
      "Detected phenomenon column: phenomene_id\n",
      "Detected vigilance column: niveau_vigilance\n",
      "\n",
      "✅ Pivot Table Created:\n",
      "   Rows (unique datetimes): 33,317\n",
      "   Columns: ['datetime', 'quantite_precipitations', 'temperature_instant', 'phenomene_1', 'phenomene_2', 'phenomene_3', 'phenomene_4', 'phenomene_5', 'phenomene_6', 'phenomene_7', 'phenomene_8', 'phenomene_9', 'phenomene_10']\n",
      "   Total columns: 13\n",
      "\n",
      "======================================================================\n",
      "PIVOT TABLE STRUCTURE\n",
      "======================================================================\n",
      "Columns: datetime, quantite_precipitations, temperature_instant, phenomene_1, ..., phenomene_10\n",
      "Shape: 33317 rows × 13 columns\n",
      "\n",
      "First 3 rows:\n",
      "              datetime  quantite_precipitations  temperature_instant  \\\n",
      "0  2022-01-01 00:00:00                      0.0                 10.5   \n",
      "1  2022-01-01 01:00:00                      0.0                 10.6   \n",
      "2  2022-01-01 02:00:00                      0.0                 10.1   \n",
      "\n",
      "  phenomene_1 phenomene_2 phenomene_3 phenomene_4 phenomene_5 phenomene_6  \\\n",
      "0        Vert        Vert        Vert        Vert        Vert        Vert   \n",
      "1        Vert        Vert        Vert        Vert        Vert        Vert   \n",
      "2        Vert        Vert        Vert        Vert        Vert        Vert   \n",
      "\n",
      "  phenomene_7 phenomene_8 phenomene_9 phenomene_10  \n",
      "0        Vert        Vert        Vert         Vert  \n",
      "1        Vert        Vert        Vert         Vert  \n",
      "2        Vert        Vert        Vert         Vert  \n",
      "\n",
      "======================================================================\n",
      "ALERT COVERAGE ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "✅ Pivot Table Created:\n",
      "   Rows (unique datetimes): 33,317\n",
      "   Columns: ['datetime', 'quantite_precipitations', 'temperature_instant', 'phenomene_1', 'phenomene_2', 'phenomene_3', 'phenomene_4', 'phenomene_5', 'phenomene_6', 'phenomene_7', 'phenomene_8', 'phenomene_9', 'phenomene_10']\n",
      "   Total columns: 13\n",
      "\n",
      "======================================================================\n",
      "PIVOT TABLE STRUCTURE\n",
      "======================================================================\n",
      "Columns: datetime, quantite_precipitations, temperature_instant, phenomene_1, ..., phenomene_10\n",
      "Shape: 33317 rows × 13 columns\n",
      "\n",
      "First 3 rows:\n",
      "              datetime  quantite_precipitations  temperature_instant  \\\n",
      "0  2022-01-01 00:00:00                      0.0                 10.5   \n",
      "1  2022-01-01 01:00:00                      0.0                 10.6   \n",
      "2  2022-01-01 02:00:00                      0.0                 10.1   \n",
      "\n",
      "  phenomene_1 phenomene_2 phenomene_3 phenomene_4 phenomene_5 phenomene_6  \\\n",
      "0        Vert        Vert        Vert        Vert        Vert        Vert   \n",
      "1        Vert        Vert        Vert        Vert        Vert        Vert   \n",
      "2        Vert        Vert        Vert        Vert        Vert        Vert   \n",
      "\n",
      "  phenomene_7 phenomene_8 phenomene_9 phenomene_10  \n",
      "0        Vert        Vert        Vert         Vert  \n",
      "1        Vert        Vert        Vert         Vert  \n",
      "2        Vert        Vert        Vert         Vert  \n",
      "\n",
      "======================================================================\n",
      "ALERT COVERAGE ANALYSIS\n",
      "======================================================================\n",
      "Total datetimes: 33,317\n",
      "Datetimes with at least 1 missing alert: 33,317 (100.00%)\n",
      "Datetimes with ALL alerts missing: 12,530 (37.61%)\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "ALERT LEVEL DISTRIBUTION\n",
      "----------------------------------------------------------------------\n",
      "Total datetimes: 33,317\n",
      "Datetimes with at least 1 missing alert: 33,317 (100.00%)\n",
      "Datetimes with ALL alerts missing: 12,530 (37.61%)\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "ALERT LEVEL DISTRIBUTION\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "UFuncTypeError",
     "evalue": "ufunc 'less' did not contain a loop with signature matching types (<class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.StrDType'>) -> None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUFuncTypeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 239\u001b[0m\n\u001b[0;32m    236\u001b[0m all_vigilance_values \u001b[38;5;241m=\u001b[39m df_pivot[phenom_cols]\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m    238\u001b[0m \u001b[38;5;66;03m# Count distribution by alert level\u001b[39;00m\n\u001b[1;32m--> 239\u001b[0m vigilance_distribution \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSeries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_vigilance_values\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue_counts\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msort_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    240\u001b[0m total_cells \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(all_vigilance_values)\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTotal alert cells analyzed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_cells\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\series.py:4069\u001b[0m, in \u001b[0;36mSeries.sort_index\u001b[1;34m(self, axis, level, ascending, inplace, kind, na_position, sort_remaining, ignore_index, key)\u001b[0m\n\u001b[0;32m   3936\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msort_index\u001b[39m(\n\u001b[0;32m   3937\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   3938\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3947\u001b[0m     key: IndexKeyFunc \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   3948\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Series \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3949\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3950\u001b[0m \u001b[38;5;124;03m    Sort Series by index labels.\u001b[39;00m\n\u001b[0;32m   3951\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4066\u001b[0m \u001b[38;5;124;03m    dtype: int64\u001b[39;00m\n\u001b[0;32m   4067\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4069\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msort_index\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4070\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4071\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4072\u001b[0m \u001b[43m        \u001b[49m\u001b[43mascending\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mascending\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4073\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4074\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkind\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4075\u001b[0m \u001b[43m        \u001b[49m\u001b[43mna_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4076\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort_remaining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort_remaining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4077\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4078\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4079\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\generic.py:5309\u001b[0m, in \u001b[0;36mNDFrame.sort_index\u001b[1;34m(self, axis, level, ascending, inplace, kind, na_position, sort_remaining, ignore_index, key)\u001b[0m\n\u001b[0;32m   5305\u001b[0m ascending \u001b[38;5;241m=\u001b[39m validate_ascending(ascending)\n\u001b[0;32m   5307\u001b[0m target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_axis(axis)\n\u001b[1;32m-> 5309\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[43mget_indexer_indexer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5310\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mascending\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_position\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort_remaining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\n\u001b[0;32m   5311\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indexer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inplace:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\sorting.py:113\u001b[0m, in \u001b[0;36mget_indexer_indexer\u001b[1;34m(target, level, ascending, kind, na_position, sort_remaining, key)\u001b[0m\n\u001b[0;32m    108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m lexsort_indexer(\n\u001b[0;32m    109\u001b[0m         codes, orders\u001b[38;5;241m=\u001b[39mascending, na_position\u001b[38;5;241m=\u001b[39mna_position, codes_given\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    110\u001b[0m     )\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;66;03m# ascending can only be a Sequence for MultiIndex\u001b[39;00m\n\u001b[1;32m--> 113\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[43mnargsort\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkind\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mascending\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mascending\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mna_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m indexer\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\sorting.py:439\u001b[0m, in \u001b[0;36mnargsort\u001b[1;34m(items, kind, ascending, na_position, key, mask)\u001b[0m\n\u001b[0;32m    437\u001b[0m     non_nans \u001b[38;5;241m=\u001b[39m non_nans[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    438\u001b[0m     non_nan_idx \u001b[38;5;241m=\u001b[39m non_nan_idx[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m--> 439\u001b[0m indexer \u001b[38;5;241m=\u001b[39m non_nan_idx[\u001b[43mnon_nans\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margsort\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkind\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ascending:\n\u001b[0;32m    441\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;31mUFuncTypeError\u001b[0m: ufunc 'less' did not contain a loop with signature matching types (<class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.StrDType'>) -> None"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ==================== CONFIGURATION ====================\n",
    "DATE_START = pd.to_datetime('2022-01-01', utc=True)\n",
    "DATE_END = pd.to_datetime('2025-12-31', utc=True)\n",
    "EXCLUDE_COLS_HOURLY = [\"numero_poste\",\"nom_usuel\",\"latitude\",\"longitude\",\"altitude\",\"duree_precipitations\", \"vent_moyen\", \"code_etat_neige\", \"charge_neige\",\n",
    "                       \"neige_au_sol\", \"code_etat_sol_sans_neige\", \"code_etat_sol_avec_neige\"]\n",
    "EXCLUDE_COLS_ALERT = [\"type_vigilance\"]\n",
    "\n",
    "def load_and_filter_data(filepath, date_col_pattern='date', date_start=DATE_START, date_end=DATE_END, exclude_cols=None, dept_number=92):\n",
    "    \"\"\"Load CSV and apply standard filters (date range, department, column exclusion).\"\"\"\n",
    "    df = pd.read_csv(filepath)\n",
    "    \n",
    "    # Auto-detect datetime column\n",
    "    dt_col = next((col for col in df.columns if date_col_pattern in col.lower() or 'time' in col.lower()), df.columns[0])\n",
    "    \n",
    "    # Convert all date columns to datetime\n",
    "    for col in df.columns:\n",
    "        if 'date' in col.lower():\n",
    "            df[col] = pd.to_datetime(df[col])\n",
    "    \n",
    "    # Normalize timezone\n",
    "    if df[dt_col].dt.tz is not None:\n",
    "        df[dt_col] = df[dt_col].dt.tz_convert('UTC')\n",
    "    else:\n",
    "        df[dt_col] = df[dt_col].dt.tz_localize('UTC')\n",
    "    \n",
    "    # Filter by date range\n",
    "    df = df[(df[dt_col] >= date_start) & (df[dt_col] <= date_end)]\n",
    "    \n",
    "    # Filter by department (keep only department 92)\n",
    "    dept_cols = [col for col in df.columns if 'department' in col.lower() or 'num_departement' in col.lower() or 'departement' in col.lower()]\n",
    "    if dept_cols:\n",
    "        dept_col = dept_cols[0]\n",
    "        df = df[df[dept_col] == dept_number]\n",
    "    \n",
    "    # Exclude columns\n",
    "    if exclude_cols:\n",
    "        df = df[[col for col in df.columns if col not in exclude_cols]]\n",
    "    \n",
    "    return df, dt_col\n",
    "\n",
    "# ==================== LOAD & FILTER DATA ====================\n",
    "print(\"Loading hourly weather data...\")\n",
    "df_hourly_filtered, datetime_col = load_and_filter_data(\n",
    "    r\"C:\\Users\\marcs\\Documents\\GitHub\\IDFm-Hackathon-Team9\\raw-data\\meteo_france_horaire_2020-2025\\meteo_france_horaire_2020-2025.csv\",\n",
    "    exclude_cols=EXCLUDE_COLS_HOURLY\n",
    ")\n",
    "print(f\"  ✓ Hourly weather: {len(df_hourly_filtered)} rows, {len(df_hourly_filtered.columns)} cols\")\n",
    "\n",
    "print(\"Loading alert data...\")\n",
    "df_alerts_filtered, datetime_col_alert = load_and_filter_data(\n",
    "    r\"C:\\Users\\marcs\\Documents\\GitHub\\IDFm-Hackathon-Team9\\raw-data\\03_Vigilance_meteo_20221127_20250722\\vigilance_data_from_2022-11-27_to_2025-07-22.csv\",\n",
    "    exclude_cols=EXCLUDE_COLS_ALERT\n",
    ")\n",
    "print(f\"  ✓ Alerts: {len(df_alerts_filtered)} rows, {len(df_alerts_filtered.columns)} cols\")\n",
    "\n",
    "# Convert datetime columns to string format for matching (project convention)\n",
    "df_hourly_filtered[datetime_col] = df_hourly_filtered[datetime_col].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "for col in df_alerts_filtered.columns:\n",
    "    if 'date' in col.lower():\n",
    "        df_alerts_filtered[col] = df_alerts_filtered[col].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# ==================== CREATE PIVOT TABLE: DATETIME × PHENOMENA ====================\n",
    "\n",
    "def create_phenomena_vigilance_pivot(df_hourly, df_alerts, datetime_col_hourly='datetime',\n",
    "                                      datetime_col_alert='datetime_debut_vigilance', \n",
    "                                      phenomene_col='phenomene_id', vigilance_col='niveau_vigilance',\n",
    "                                      date_start_col='date_debut_vigilance', date_end_col='date_fin_vigilance',\n",
    "                                      quantite_precipitations='quantite_precipitations',\n",
    "                                      temperature_instant='temperature_instant',\n",
    "                                      num_phenomena=10, default_vigilance='Vert'):\n",
    "    \"\"\"\n",
    "    Create a pivot table with datetime rows, weather columns, and phenomena (1-10) columns.\n",
    "    Each cell contains the niveau_vigilance for that datetime and phenomenon, plus weather data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_hourly : DataFrame\n",
    "        Hourly weather data with datetime column and weather columns\n",
    "    df_alerts : DataFrame\n",
    "        Alert data with phenomenon, vigilance level, and date range columns\n",
    "    datetime_col_hourly : str\n",
    "        Name of datetime column in df_hourly\n",
    "    datetime_col_alert : str\n",
    "        Name of datetime alert column (unused, kept for compatibility)\n",
    "    phenomene_col : str\n",
    "        Name of phenomenon ID column in df_alerts\n",
    "    vigilance_col : str\n",
    "        Name of vigilance level column in df_alerts\n",
    "    date_start_col : str\n",
    "        Name of alert start datetime column in df_alerts\n",
    "    date_end_col : str\n",
    "        Name of alert end datetime column in df_alerts\n",
    "    quantite_precipitations : str\n",
    "        Name of precipitation quantity column in df_hourly\n",
    "    temperature_instant : str\n",
    "        Name of instantaneous temperature column in df_hourly\n",
    "    num_phenomena : int\n",
    "        Number of phenomena columns to create (1 to num_phenomena)\n",
    "    default_vigilance : str\n",
    "        Default value when no alert matches (e.g., 'Vert', 'Green', None)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame\n",
    "        Pivot table with columns: ['datetime', 'quantite_precipitations', 'temperature_instant', 'phenomene_1', ..., 'phenomene_N']\n",
    "    \"\"\"\n",
    "    \n",
    "    # Weather columns to include from df_hourly\n",
    "    weather_cols = [quantite_precipitations, temperature_instant]\n",
    "    \n",
    "    # Get unique datetimes from hourly data (as strings)\n",
    "    unique_datetimes = sorted(df_hourly[datetime_col_hourly].unique())\n",
    "    \n",
    "    # Initialize result dataframe\n",
    "    pivot_data = []\n",
    "    \n",
    "    # For each unique datetime\n",
    "    for dt_str in unique_datetimes:\n",
    "        row = {'datetime': dt_str}\n",
    "        \n",
    "        # Convert to datetime for comparison\n",
    "        dt = pd.to_datetime(dt_str)\n",
    "        \n",
    "        # Add weather columns\n",
    "        for weather_col in weather_cols:\n",
    "            # Get the weather value for this datetime\n",
    "            weather_values = df_hourly[df_hourly[datetime_col_hourly] == dt_str][weather_col]\n",
    "            if len(weather_values) > 0:\n",
    "                row[weather_col] = weather_values.iloc[0]\n",
    "            else:\n",
    "                row[weather_col] = None\n",
    "        \n",
    "        # Convert alert dates to datetime for comparison\n",
    "        alert_start = pd.to_datetime(df_alerts[date_start_col])\n",
    "        alert_end = pd.to_datetime(df_alerts[date_end_col])\n",
    "        \n",
    "        # For each phenomenon (1 to num_phenomena)\n",
    "        for phenom_id in range(1, num_phenomena + 1):\n",
    "            # Find matching alert for this datetime and phenomenon\n",
    "            matching_alerts = df_alerts[\n",
    "                (alert_start <= dt) & \n",
    "                (alert_end >= dt) & \n",
    "                (df_alerts[phenomene_col] == phenom_id)\n",
    "            ]\n",
    "            \n",
    "            # Get vigilance level or use default\n",
    "            if len(matching_alerts) > 0:\n",
    "                # If multiple alerts match, take the first one (or could aggregate)\n",
    "                vigilance = matching_alerts.iloc[0][vigilance_col]\n",
    "            else:\n",
    "                vigilance = default_vigilance\n",
    "            \n",
    "            row[f'phenomene_{phenom_id}'] = vigilance\n",
    "        \n",
    "        pivot_data.append(row)\n",
    "    \n",
    "    df_pivot = pd.DataFrame(pivot_data)\n",
    "    return df_pivot\n",
    "\n",
    "\n",
    "# Create pivot table\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CREATING PHENOMENA × VIGILANCE PIVOT TABLE WITH WEATHER DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# First, convert alert datetimes back to datetime for the pivot function\n",
    "df_alerts_for_pivot = df_alerts_filtered.copy()\n",
    "for col in df_alerts_for_pivot.columns:\n",
    "    if 'date' in col.lower():\n",
    "        df_alerts_for_pivot[col] = pd.to_datetime(df_alerts_for_pivot[col])\n",
    "\n",
    "# Detect phenomenon and vigilance columns\n",
    "phenomene_col = next((col for col in df_alerts_for_pivot.columns if 'phenomene' in col.lower()), 'phenomene_id')\n",
    "vigilance_col = next((col for col in df_alerts_for_pivot.columns if 'niveau' in col.lower()), 'niveau_vigilance')\n",
    "\n",
    "print(f\"Detected phenomenon column: {phenomene_col}\")\n",
    "print(f\"Detected vigilance column: {vigilance_col}\")\n",
    "\n",
    "# Create pivot table with weather columns (quantite_precipitations and temperature_instant)\n",
    "df_pivot = create_phenomena_vigilance_pivot(\n",
    "    df_hourly=df_hourly_filtered,\n",
    "    df_alerts=df_alerts_for_pivot,\n",
    "    datetime_col_hourly=datetime_col,\n",
    "    phenomene_col=phenomene_col,\n",
    "    vigilance_col=vigilance_col,\n",
    "    date_start_col='date_debut_vigilance',\n",
    "    date_end_col='date_fin_vigilance',\n",
    "    quantite_precipitations='quantite_precipitations',\n",
    "    temperature_instant='temperature_instant',\n",
    "    num_phenomena=10,\n",
    "    default_vigilance='Vert'\n",
    ")\n",
    "\n",
    "print(f\"\\n✅ Pivot Table Created:\")\n",
    "print(f\"   Rows (unique datetimes): {len(df_pivot):,}\")\n",
    "print(f\"   Columns: {list(df_pivot.columns)}\")\n",
    "print(f\"   Total columns: {len(df_pivot.columns)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PIVOT TABLE STRUCTURE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Columns: datetime, quantite_precipitations, temperature_instant, phenomene_1, ..., phenomene_10\")\n",
    "print(f\"Shape: {df_pivot.shape[0]} rows × {df_pivot.shape[1]} columns\")\n",
    "print(\"\\nFirst 3 rows:\")\n",
    "print(df_pivot.head(3))\n",
    "\n",
    "# ==================== ALERT COVERAGE ANALYSIS ====================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ALERT COVERAGE ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Identify phenomenon columns (all columns except 'datetime' and weather columns)\n",
    "phenom_cols = [col for col in df_pivot.columns if col.startswith('phenomene_')]\n",
    "\n",
    "# Count rows with at least one missing alert (contains 'Vert')\n",
    "rows_with_missing = (df_pivot[phenom_cols] == 'Vert').any(axis=1).sum()\n",
    "pct_with_missing = (rows_with_missing / len(df_pivot)) * 100\n",
    "\n",
    "# Count rows with ALL alerts missing (all 'Vert')\n",
    "rows_all_missing = (df_pivot[phenom_cols] == 'Vert').all(axis=1).sum()\n",
    "pct_all_missing = (rows_all_missing / len(df_pivot)) * 100\n",
    "\n",
    "print(f\"Total datetimes: {len(df_pivot):,}\")\n",
    "print(f\"Datetimes with at least 1 missing alert: {rows_with_missing:,} ({pct_with_missing:.2f}%)\")\n",
    "print(f\"Datetimes with ALL alerts missing: {rows_all_missing:,} ({pct_all_missing:.2f}%)\")\n",
    "\n",
    "# ==================== ALERT LEVEL DISTRIBUTION ====================\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"ALERT LEVEL DISTRIBUTION\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Flatten all vigilance values from phenomena columns\n",
    "all_vigilance_values = df_pivot[phenom_cols].values.flatten()\n",
    "\n",
    "# Count distribution by alert level\n",
    "vigilance_distribution = pd.Series(all_vigilance_values).value_counts().sort_index()\n",
    "total_cells = len(all_vigilance_values)\n",
    "\n",
    "print(f\"\\nTotal alert cells analyzed: {total_cells:,}\\n\")\n",
    "\n",
    "# Define alert level order\n",
    "alert_level_order = ['Vert', 'Jaune', 'Orange', 'Rouge', 'Magenta']\n",
    "for level in alert_level_order:\n",
    "    if level in vigilance_distribution.index:\n",
    "        count = vigilance_distribution[level]\n",
    "        pct = (count / total_cells) * 100\n",
    "        bar = \"█\" * int(pct / 2)  # Scale bar to fit terminal\n",
    "        print(f\"{level:12} : {count:8,} ({pct:6.2f}%) {bar}\")\n",
    "\n",
    "# Show any other levels not in standard order\n",
    "for level, count in vigilance_distribution.items():\n",
    "    if level not in alert_level_order:\n",
    "        pct = (count / total_cells) * 100\n",
    "        bar = \"█\" * int(pct / 2)\n",
    "        print(f\"{level:12} : {count:8,} ({pct:6.2f}%) {bar}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"Sample of 5 random rows:\")\n",
    "print(\"-\"*70)\n",
    "sample_rows = df_pivot.sample(n=min(5, len(df_pivot)), random_state=42)\n",
    "print(sample_rows.to_string())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
